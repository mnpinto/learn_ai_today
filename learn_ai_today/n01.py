# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_linear_regression.ipynb (unless otherwise specified).

__all__ = ['LinearRegression', 'fit', 'fit2', 'GeneralFit']

# Cell
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from fastai.vision import Module
import matplotlib.pyplot as plt
from matplotlib import animation, rc
from IPython.display import HTML

# Cell
# Defining the model
class LinearRegression(Module):
    def __init__(self, number_of_inputs, number_of_outputs):
        self.linear = nn.Linear(number_of_inputs, number_of_outputs)

    def forward(self, x):
        return self.linear(x)

# Cell
# Defining the fit function
def fit(inputs, targets, model, criterion, optimizer, num_epochs):
    loss_history = [] # to save the loss at each epoch.

    for epoch in range(num_epochs):
        # forward pass
        out = model(inputs)
        loss = criterion(out, targets)

        # backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # store value of loss
        loss_history.append(loss.item())

    print('Epoch[{}/{}], loss:{:.6f}'.format(epoch+1, num_epochs, loss.item()))
    return loss_history

# Cell
def fit2(inputs, targets, model, criterion, optimizer, num_epochs):
    loss_history = [] # to save the loss at each epoch.
    out_history = [] # to save the parameters at each epoch
    for ii, epoch in enumerate(range(num_epochs)):
        # forward
        out = model(inputs)
        loss = criterion(out, targets)

        # backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        loss_history.append(loss.item())

        if ii == 0:
            out_history = out.detach().numpy()
        else:
            out_history = np.concatenate((out_history, out.detach().numpy()), axis=-1)

    print('Epoch[{}/{}], loss:{:.6f}'.format(epoch+1, num_epochs, loss.item()))
    return loss_history, out_history

# Cell
class GeneralFit(Module):
    def __init__(self, input_size, output_size, hidden_size=100):
        self.linear_in  = nn.Linear(input_size, hidden_size)
        self.hidden     = nn.Linear(hidden_size, hidden_size)
        self.linear_out = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.relu(self.linear_in(x))
        x = torch.relu(self.hidden(x))
        x = self.linear_out(x)
        return x